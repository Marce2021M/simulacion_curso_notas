#+TITLE: EST-24107: Simulación
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Diagnósticos MCMC~
#+STARTUP: showall
:LATEX_PROPERTIES:
#+SETUPFILE: ~/.emacs.d/templates/latex/handout.org
#+EXPORT_FILE_NAME: ../docs/09-diagnosticos.pdf
:END:
#+EXCLUDE_TAGS: toc latex
#+PROPERTY: header-args:R :session diagnosticos :exports both :results output org :tangle ../rscripts/09-diagnosticos.R :mkdirp yes :dir ../ :eval never

#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Otoño, 2023 | Diagnósticos MCMC.\\
*Objetivo*. Estudiaremos diagnósticos típicos de métodos de simulación Markoviana
con el objetivo de poder identificar problemas y posibles soluciones para
simulaciones deficientes. \\
*Lectura recomendada*: Para una revisión de los diagnósticos de MCMC busca la
sección 11.4 de citep:Gelman2014a. El Capítulo 3 de citep:Cressie2015 tiene una
revisión muy bonita de series de tiempo que es útil para algunos términos.
#+END_NOTES

#+begin_src R :exports none :results none
  ## Setup ---------------------------------------------------------------------
  library(tidyverse)
  library(patchwork)
  library(scales)

  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 4)
  ## Problemas con mi consola en Emacs
  options(pillar.subtle = FALSE)
  options(rlang_backtrace_on_error = "none")
  options(crayon.enabled = FALSE)
  options(width=60)

  ## Para el tema de ggplot
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)
  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), axis.text = element_blank())
  sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
#+end_src

* Contenido                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#introducción][Introducción]]
- [[#diagnósticos-generales][Diagnósticos generales]]
  - [[#monitoreo-de-convergencia][Monitoreo de convergencia]]
    - [[#datos-cantantes-de-ópera][Datos: Cantantes de ópera]]
  - [[#monitoreo-de-mezcla-dentro-y-entre-cadenas][Monitoreo de mezcla dentro y entre cadenas]]
  - [[#número-efectivo-de-simulaciones][Número efectivo de simulaciones]]
  - [[#relación-con-error-monte-carlo][Relación con error Monte Carlo]]
- [[#el-estado-del-arte][El estado del arte]]
:END:

* Introducción

El avance en poder computacional ha permitido la proliferación de métodos Bayesianos. El poder generar cadenas de Markov es múltiples procesadores nos ayuda a relajar los requisitos y ayuda a explotar los recursos computacionales disponibles. 

#+begin_src R :exports none :results none
  library(mvtnorm)
  library(R6)
  ModeloNormalMultivariado <-
    R6Class("ProbabilityModel",
            list(
              mean = NA,
              cov  = NA, 
              initialize = function(mu = 0, sigma = 1){
                self$mean = mu
                self$cov  = sigma |> as.matrix()
              }, 
              sample = function(n = 1){
                rmvnorm(n, mean = self$mean, sigma = self$cov)              
              },
              density = function(x, log = TRUE){
                dmvnorm(x, self$mean, self$cov, log = log)              
              }           
            ))
#+end_src

#+begin_src R :exports none :results none
  ### Muestreador Metropolis-Hastings -------------------------
  crea_metropolis_hastings <- function(objetivo, muestreo){
    ## Este muestreador aprovecha la simetría de la propuesta 
    function(niter, x_start){
      ## Empezamos en algun lugar
      estado <- x_start
      ndim <- length(estado) 
      muestras <- matrix(nrow = niter, ncol = ndim + 1)      
      muestras[1,2:(ndim+1)] <- estado
      muestras[1,1] <- 1
      for (ii in 2:niter){
        propuesta   <- estado + muestreo$sample()
        log_pi_propuesta <- objetivo$density(propuesta)
        log_pi_estado    <- objetivo$density(estado)
        log_alpha <- log_pi_propuesta - log_pi_estado

        if (log(runif(1)) < log_alpha) {
          muestras[ii, 1] <- 1 ## Aceptamos
          muestras[ii, 2:(ndim+1)] <- propuesta
        } else {
          muestras[ii, 1] <- 0 ## Rechazamos
          muestras[ii, 2:(ndim+1)] <- estado
        }
        estado <- muestras[ii, 2:(ndim+1)]
      }
      if (ndim == 1) {colnames(muestras) <- c("accept", "value")}
      muestras
    }
  }

#+end_src

#+begin_src R :exports none :results none
    set.seed(108727)
    mu <- c(0, 0)
    Sigma <- matrix(c(1, .75, .75, 1), nrow = 2)
    objetivo <- ModeloNormalMultivariado$new(mu, Sigma)
    muestreo <- ModeloNormalMultivariado$new(c(0,0),  .05 * diag(2))

    muestras <- tibble(id = factor(1:5), x1 = c(-2, 2, 2, -2, 0), x2 = c(2, -2, 2, -2, 0)) |>
      nest(x_start   = c(x1,x2)) |>
      mutate(cadenas = map(x_start, function(x0){
        mcmc <- crea_metropolis_hastings(objetivo, muestreo)
        mcmc(1000, c(x0$x1, x0$x2)) |>
          as_tibble() |>
          mutate(iter = 1:1000)
      }))
#+end_src

#+REVEAL: split
Cuando generamos una muestra de la distribución objetivo usando
MCMC, sin importar el método (Metrópolis, Gibbs, HMC), buscamos que:

#+REVEAL: split
- Los valores simulados ~no estén influenciados~ por el valor inicial (arbitrario)
  y deben explorar todo el rango de la distribución objetivo.
- Debemos tener ~suficientes simulaciones~ de tal manera que las estimaciones sean
  precisas y estables.
- Queremos tener ~métodos y resúmenes informativos~ que nos ayuden diagnosticar
  correctamente el desempeño de nuestas simulaciones.

#+REVEAL: split
En la ~práctica~ intentamos cumplir lo más posible estos objetivos. Debemos de
tener un criterio para considerar cadenas de ~longitud finita~ y ~evaluar la calidad~ de las
simulaciones.

#+REVEAL: split
Primero estudiaremos ~diagnósticos generales~ para métodos que utilicen MCMC y
después mencionaremos particularidades del método de simulación HMC.

* Diagnósticos generales

Una forma que tenemos de evaluar la (o identificar la falta de) convergencia es
considerar distintas secuencias independientes. 

#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/cadenas-multiples.jpeg :exports results :results output graphics file
    g.corta <- muestras |>
      unnest(cadenas) |>
      filter(iter <= 50) |>
      ggplot(aes(V2, V3, color = id)) +
      geom_path() + geom_point(size = .3) +
      geom_point(data = muestras |> unnest(x_start), aes(x1, x2), color = 'red') + 
      xlab(expression(x[1])) + ylab(expression(x[2])) + 
      sin_lineas + sin_leyenda + ylim(-3,3) + xlim(-3,3)


    g.completa <- muestras |>
      unnest(cadenas) |>
      ggplot(aes(V2, V3, color = id)) +
      geom_path() + geom_point(size = .3) +
      geom_point(data = muestras |> unnest(x_start), aes(x1, x2), color = 'red') + 
      xlab(expression(x[1])) + ylab(expression(x[2])) + 
      sin_lineas + sin_leyenda + ylim(-3,3) + xlim(-3,3)

    g.conjunta <- muestras |>
      unnest(cadenas) |>
      ggplot(aes(V2, V3)) +
      geom_point(size = .3) +
      geom_point(data = muestras |> unnest(x_start), aes(x1, x2), color = 'red') + 
      xlab(expression(x[1])) + ylab(expression(x[2])) + 
      sin_lineas + sin_leyenda + ylim(-3,3) + xlim(-3,3)

  g.objetivo <- objetivo$sample(4000) |>
    as_tibble() |>
    ggplot(aes(V1, V2)) +
      geom_point(size = .3) +
      xlab(expression(x[1])) + ylab(expression(x[2])) + 
      sin_lineas + sin_leyenda + ylim(-3,3) + xlim(-3,3)

    (g.corta + g.completa) / (g.conjunta + g.objetivo)
#+end_src
#+caption: Distintas cadenas de Markov. 
#+RESULTS:
[[file:../images/cadenas-multiples.jpeg]]


#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/traza-diagnosticos.jpeg :exports results :results output graphics file
  muestreo <- ModeloNormalMultivariado$new(c(0,0),  10 * diag(2))

  muestras.mal <- tibble(id = factor(1:5), x1 = c(-2, 2, 2, -2, 0), x2 = c(2, -2, 2, -2, 0)) |>
    nest(x_start   = c(x1,x2)) |>
    mutate(cadenas = map(x_start, function(x0){
      mcmc <- crea_metropolis_hastings(objetivo, muestreo)
      mcmc(1000, c(x0$x1, x0$x2)) |>
        as_tibble() |>
        mutate(iter = 1:1000)
    }))

  g1 <- muestras |>
    unnest(cadenas) |>
    ggplot(aes(iter, V2, color = id)) +
    geom_line() + sin_lineas + sin_leyenda +
    ylab(expression(x[1]))


  g2 <- muestras.mal |>
    unnest(cadenas) |>
    ggplot(aes(iter, V2, color = id)) +
    geom_line() + sin_lineas + sin_leyenda +
    ylab(expression(x[1]))

  g1/g2
#+end_src
#+caption: Trayectorias de simulación para $X_1$. 
#+RESULTS:
[[file:../images/traza-diagnosticos.jpeg]]

** Monitoreo de convergencia

~Burn-in e iteraciones iniciales~. En primer lugar, en muchas ocasiones las
condiciones iniciales de las cadenas las escogemos de tal forma que 
que son /atípicos/ en relación a la distribución objetivo.

#+BEGIN_NOTES
Estrategias de selección de puntos iniciales pueden ser valores aleatorios de la
previa o perturbaciones aleatorias a estimadores $\textsf{MLE}$.
#+END_NOTES

#+REVEAL: split
Correr varias cadenas en puntos dispersos tienen la ventaja de explorar desde
distintas regiones de la distribución objetivo. Eventualmente, esperamos que todas las
/cadenas/ ~mezclen bien~ y ~representen realizaciones independientes~ del mismo
proceso estócastico (Markoviano).

#+REVEAL: split
Para contrarrestar la dependencia en los distintos puntos iniciales se descarta 
parte de la cadena en un periodo inicial (periodo de calentamiento).

*** Datos: Cantantes de ópera

#+begin_src R :exports code :results none
  ## Datos: cantantes de opera -----------------------
  set.seed(3413)
  cantantes <- lattice::singer |>
    mutate(estatura_cm = round(2.54 * height)) |>
    filter(str_detect(voice.part, "Tenor")) |>
    select(voice.part, estatura_cm) |>
    sample_n(20)
#+end_src

#+begin_src R :exports results :results org
   cantantes |> tibble() |> print(n = 3)
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 20 × 2
  voice.part estatura_cm
  <fct>            <dbl>
1 Tenor 1            178
2 Tenor 2            173
3 Tenor 1            165
# ℹ 17 more rows
# ℹ Use `print(n = ...)` to see more rows
#+end_src


#+REVEAL: split
Denotamos por $x_i$ la estatura de tenores (cantantas de ópera). Asumimos un modelo Normal con parámetros poblacionales no observados:  $\mu$ y $\sigma$. El modelo previo lo asumimos como
\begin{gather}
\mu | \sigma \sim \mathsf{Normal}\left(\mu_0, \frac{\sigma}{n_0}\right)\,,\\
\sigma^{-1} \sim \mathsf{Gamma}(a_0, b_0)\,.
\end{gather}


#+begin_src R :exports none :results none
  ModeloNormal <-
    R6Class("PosteriorProbabilityModel",
            list(
              observaciones = NA,
              mu_0 = NA, n_0 = NA, a = NA, b = NA,
              initialize = function(x = 0){
                ## Observaciones
                self$observaciones <- x
                ## Previa
                self$mu_0 <- 175
                self$n_0  <- 5
                self$a    <- 3
                self$b    <- 140
              },
              density = function(theta, log = TRUE){
                theta <- matrix(theta, nrow = 1)
                verosimilitud <- sum(dnorm(self$observaciones, theta[1], sd = theta[2], log = log))
                previa <- dnorm(theta[1], self$mu_0, sd = theta[2]/sqrt(self$n_0), log = log) +
                  dgamma(1/(theta[2]**2), self$a, self$b, log = log)
                verosimilitud + previa 
              }           
            ))

  objetivo <- ModeloNormal$new(cantantes$estatura_cm)
  muestreo <- ModeloNormalMultivariado$new(c(0,0),  0.50 * diag(2))
#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/cantantes-muestras.jpeg :exports results :results output graphics file
  set.seed(108727)
  mcmc <- crea_metropolis_hastings(objetivo, muestreo)

  muestras.cantantes <-  mcmc(5000, c(162, 3)) |>
    as_tibble() |>
    mutate(mu = V2, sigma = V3, iter = 1:n())

  muestras.cantantes |>
    ggplot(aes(mu, sigma, color = iter)) +
    geom_line(alpha = .2) +geom_point(size = 4, alpha = .4) + 
    sin_lineas 
 #+end_src
#+caption: Cadena de Markov simulando de la posterior como distribución objetivo. 
 #+RESULTS:
 [[file:../images/cantantes-muestras.jpeg]]

En esta simulación es evidente$^\dagger$ que necesitamos descartar una parte inicial de la simulación.

#+REVEAL: split
citet:Gelman2014a recomiendan descartar la mitad de las iteraciones de cada una de las cadenas
que se simularon. Para problemas en dimensiones altas, incluso se podría esperar 
descartar hasta un $80\%$ de simulaciones (en especial para métodos basados en
Metropolis-Hastings).

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/trayectorias-cantantes.jpeg :exports results :results output graphics file
   cadenas.cantantes <- tibble(cadena  = factor(1:4),
          mu_start    = rnorm(4, 160, 20),
          sigma_start = runif(4, 0, 20)) |>
     nest(inicial = c(mu_start, sigma_start)) |>
     mutate(cadenas = map(inicial, function(x0){
       mcmc(2500, c(x0$mu_start, x0$sigma_start)) |>
         as_tibble() |>
         mutate(mu = V2, sigma = V3, iter = 1:n())
     }))

  cadenas.cantantes |>
     unnest(cadenas) |>
     pivot_longer(cols = mu:sigma) |>
     ggplot(aes(iter, value, color = cadena)) +
     geom_line() +
     facet_wrap(~name, ncol = 1, scales = "free_y") +
     sin_lineas
#+end_src
#+caption: Trayectorias con dependencias iniciales.
#+RESULTS:
[[file:../images/trayectorias-cantantes.jpeg]]

#+REVEAL: split
#+HEADER: :width 1200 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/trayectorias-estacionarias-cantantes.jpeg :exports results :results output graphics file
  cadenas.cantantes |>
    unnest(cadenas) |>
    filter(iter >= 1000) |> 
    pivot_longer(cols = mu:sigma) |>
    ggplot(aes(iter, value, color = cadena)) +
    geom_line() +
    facet_wrap(~name, ncol = 1, scales = "free_y") +
    sin_lineas
#+end_src
#+caption: Trayectorias estacionarias.
#+RESULTS:
[[file:../images/trayectorias-estacionarias-cantantes.jpeg]]

** Monitoreo de mezcla /dentro/ y /entre/ cadenas

Podemos utilizar ~todas~ las simulaciones como si vinieran de una sola cadena (argumentando por estacionariedad)

#+begin_src R :exports both :results org 
  cadenas.cantantes |>
    unnest(cadenas) |>
    filter(iter > 100) |> 
    summarise(.estimate = mean(sigma), .variance = var(sigma),
              .error_mc = sqrt(.variance/n()))
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 1 × 3
  .estimate .variance .error_mc
      <dbl>     <dbl>     <dbl>
1      7.11      4.14    0.0208
#+end_src

#+REVEAL: split
Sin embargo, al calcular la varianza como si fueran 4 cadenas independientes vemos que nuestro estimador del error Monte Carlo es mucho mas elevado de lo que esperamos ¿por qué?

#+begin_src R :exports both :results org 
  cadenas.cantantes |>
   unnest(cadenas) |>
   filter(iter > 100) |> 
   group_by(cadena) |> 
   summarise(media = mean(sigma), varianza = var(sigma)) |>
   summarise(.estimate = mean(media), .error_mc = sd(media))
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 1 × 2
  .estimate .error_mc
      <dbl>     <dbl>
1      7.11     0.272
#+end_src

#+REVEAL: split
Al inspeccionar cada cadena tenemos los siguientes resúmenes

#+begin_src R :exports both :results org 
  cadenas.cantantes |>
   unnest(cadenas) |>
   filter(iter > 100) |> 
   group_by(cadena) |> 
   summarise(media = mean(sigma), varianza = var(sigma))
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 4 × 3
  cadena media varianza
  <fct>  <dbl>    <dbl>
1 1       7.34     7.74
2 2       6.93     2.35
3 3       6.82     1.13
4 4       7.35     5.12
#+end_src

#+REVEAL: split
Podemos partir cada cadena a la mitad y calcular nuestra estimación del error
Monte Carlo. Ahora tenemos  $8$ cadenas que /esperamos/ sean ~estacionarias~
(/idénticamente distribuidas/).

#+begin_src R :exports results :results org 
  cadenas.cantantes |>
   unnest(cadenas) |>
   filter(iter > 100) |>
   group_by(cadena) |>
   mutate(.draw = 1:n(), .particion = ifelse(.draw >= n()/2, 1, 0)) |>
   group_by(cadena, .particion) |>
   summarise(media = mean(sigma), varianza = var(sigma), .groups = "drop") |>
   summarise(.estimate = mean(media), .error_mc = sd(media))
#+end_src

#+RESULTS:
#+begin_src org
# A tibble: 1 × 2
  .estimate .error_mc
      <dbl>     <dbl>
1      7.11     0.418
#+end_src

#+REVEAL: split
Nota cómo está sucediendo algo contraintuitivo. Tenemos mas observaciones
(pasamos de 1 cadena a 8) y el error Monte Carlo no decrece.  Lo cual indica que
nuestras cadenas realmente no han terminado de converger y tienen comportamiento
distinto aunque en promedio parecen estar cercanas.

#+REVEAL: split
Gelman y diversos de sus coatures han desarollado un diagnóstico numérico para evaluar
implementaciones de MCMC al considerar múltiples cadenas. Aunque éste
estadístico se ha ido refinando con los años, su desarrollo muestra 
un entendimiento gradual de éstos métodos en la práctica. La
medida $\hat{R}$ se conoce como el ~factor de reducción potencial de escala~.

#+REVEAL: split
El estadístico $\hat R$ pretende ser una estimación de la posible ~reducción en
la longitud~ de un intervalo de confianza si las simulaciones continuaran
infinitamente.

#+REVEAL: split
La $\hat R$ estudia de manera simultánea ~la mezcla~ de todas
las cadenas (cada cadena, y fracciones de ella, deberían de haber transitado el
soporte de la distribución objetivo) y ~estacionalidad~ (de haberse logrado cada
mitad de una cadena deberían de poseer las mismas estadísticas).

#+REVEAL: split
La estrategia es descartar la ~primera mitad~ de cada cadena. El resto lo volvemos
a dividir en dos y utilizamos cada fracción como si fuera una cadena independiente$^\dagger$.

#+HEADER: :width 900 :height 300 :R-dev-args bg="transparent"
#+begin_src R :file images/split-cadenas.jpeg :exports results :results output graphics file
cadenas.cantantes |>
    unnest(cadenas) |>
    filter(iter < 300) |>
    ggplot(aes(x = iter, y = mu, color = cadena)) + 
    geom_path() +  sin_lineas + 
    annotate("rect", xmin = 0, xmax = 225, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("rect", xmin = 0, xmax = 150, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("text", x = c(75, 187.5,262.5),
             y = rep(145, 3), 
             label = c("burn-in", "sub 1", "sub 2"))
#+end_src
#+caption: Separación de simulaciones para cálculo de $\hat R$. 
#+RESULTS:
[[file:../images/split-cadenas.jpeg]]


#+REVEAL: split
Denotemos por $m$ el número de cadenas simuladas y por $n$ el número de 
simulaciones dentro de cada cadena. Cada una de las ~cantidades escalares de
interés~ las denotamos por $\phi$. Éstas pueden ser los parámetros originales
$\theta$ o alguna otra cantidad derivada $\phi = f(\theta)$.


#+REVEAL: split
Ahora denotemos por $\phi_{ij}$ las simulaciones que tenemos disponibles con $i
= 1, \ldots, n$, y $j = 1, \ldots, m$. Calculamos $B$ y $W$, la variabilidad
~entre~ (/between/) y ~dentro~ (/within/) cadenas, respectivamente, por medio de
\begin{align}
W &= \frac1m \sum_{j = 1}^m s_j^2, \quad \text{con} \quad s_j^2 = \frac{1}{n-1}\sum_{i = 1}^n (\phi_{ij} - \bar \phi_{\cdot j})^2, \quad \text{donde} \quad \bar \phi_{\cdot j} = \frac1n \sum_{i = 1}^n \phi_{ij}, \\
B &= \frac{n}{m-1}\sum_{j = 1}^m (\bar \phi_{\cdot j} - \bar \phi_{\cdot \cdot})^2, \quad \text{donde} \quad \bar \phi_{\cdot \cdot} = \frac1m \sum_{j = 1}^m \bar \phi_{\cdot j}.
\end{align}


#+BEGIN_NOTES
La varianza entre cadenas, $B$, se multiplica por $n$ dado que ésta se calcula
por medio de promedios y sin este factor de corrección no reflejaría la
variabilidad de las cantidades de interés $\phi$. 
#+END_NOTES

#+REVEAL: split
La varianza de $\phi$ se puede estimar por medio de 
\begin{align}
\hat{\mathbb{V}}(\phi)^+ = \frac{n -1}{n} W + \frac{1}{n} B \, .
\end{align}

Este estimador ~sobre-estima~ la varianza pues los puntos iniciales
pueden estar sobre-dispersos, mientras que es un ~estimador insesgado~ una vez
que se haya alcanzado el estado estacionario (realizaciones de la distribución
objetivo)

#+REVEAL: split
Por otro lado, la varianza estimada por $W$ será un sub-estimador pues podría
ser el caso de que cada cadena no ha tenido la oportunidad de recorrer todo el
soporte de la distribución. En el límite $n \to \infty$, el valor esperado de
$W$ aproxima $\mathbb{V}(\phi)$. 

#+REVEAL: split
Se utiliza como diagnostico el factor por el cual la escala de la
distribución actual de $\phi$ se puede reducir si se continua con el
procedimiento en el límite $n \to \infty$. Esto es, 
$$\hat{R} = \sqrt{\frac{\hat{\mathbb{V}}(\phi)^+}{W}}\,,$$
por construcción converge a 1 cuando $n \to \infty$.

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/diagnosticos-rhat-cantantes.jpeg :exports results :results output graphics file
  diagnosticos.rhat.short <- cadenas.cantantes |>
    unnest(cadenas) |>
    filter(iter < 200) |>
    filter(iter > max(iter)/2) |>
    mutate(cadena = paste(cadena, ifelse(iter <= (max(iter) + min(iter))/2, 
                                         'a', 'b'), sep = "")) |>
    pivot_longer(mu:sigma, names_to = "parametro", values_to = "valor") |>
    group_by(parametro, cadena) |>
    summarise(media = mean(valor), num = n(), sigma2 = var(valor)) |>
    summarise(N = first(num), 
              M = n_distinct(cadena), 
              B = N * var(media), 
              W = mean(sigma2), 
              V_hat = ((N-1)/N) * W + B/N,
              R_hat = sqrt(V_hat/W)) 

  g.mu <- cadenas.cantantes |>
    unnest(cadenas) |>
    filter(iter < 200) |>
    ggplot(aes(x = iter, y = mu, color = cadena)) + 
    geom_path() + sin_leyenda + sin_lineas + 
    ggtitle(paste("Rhat: ", round((diagnosticos.rhat.short |> pull(R_hat))[1], 3), sep = "")) + 
    annotate("rect", xmin = 0, xmax = 150, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("rect", xmin = 0, xmax = 100, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("text", x = c(50, 125, 175),
             y = rep(145, 3), 
             label = c("burn-in", "sub 1", "sub 2"))

  g.sigma <- cadenas.cantantes |>
    unnest(cadenas) |>
    filter(iter < 200) |>
    ggplot(aes(x = iter, y = sigma, color = cadena)) + 
    geom_path() + sin_leyenda + sin_lineas + 
    ggtitle(paste("Rhat: ", round((diagnosticos.rhat.short |> pull(R_hat))[2], 3), sep = "")) + 
    annotate("rect", xmin = 0, xmax = 150, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("rect", xmin = 0, xmax = 100, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("text", x = c(50, 125, 175),
             y = rep(5, 3), 
             label = c("burn-in", "sub 1", "sub 2"))

  g.mu / g.sigma
#+end_src
#+caption: Diágnostico de reducción de escala. Sugerencia: generar mas simulaciones. 
#+RESULTS:
[[file:../images/diagnosticos-rhat-cantantes.jpeg]]

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/diagnosticos-rhat-cantantes-estacionario.jpeg :exports results :results output graphics file
  diagnosticos.rhat.short <- cadenas.cantantes |>
    unnest(cadenas) |>
    filter(iter < 600) |>
    filter(iter > max(iter)/2) |>
    mutate(cadena = paste(cadena, ifelse(iter <= (max(iter) + min(iter))/2, 
                                         'a', 'b'), sep = "")) |>
    pivot_longer(mu:sigma, names_to = "parametro", values_to = "valor") |>
    group_by(parametro, cadena) |>
    summarise(media = mean(valor), num = n(), sigma2 = var(valor)) |>
    summarise(N = first(num), 
              M = n_distinct(cadena), 
              B = N * var(media), 
              W = mean(sigma2), 
              V_hat = ((N-1)/N) * W + B/N,
              R_hat = sqrt(V_hat/W)) 

  g.mu <- cadenas.cantantes |>
    unnest(cadenas) |>
    filter(iter < 600) |>
    ggplot(aes(x = iter, y = mu, color = cadena)) + 
    geom_path() + sin_leyenda + sin_lineas + 
    ggtitle(paste("Rhat: ", round((diagnosticos.rhat.short |> pull(R_hat))[1], 3), sep = "")) + 
    annotate("rect", xmin = 0, xmax = 450, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("rect", xmin = 0, xmax = 300, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("text", x = c(150, 375, 525),
             y = rep(145, 3), 
             label = c("burn-in", "sub 1", "sub 2"))

  g.sigma <- cadenas.cantantes |>
    unnest(cadenas) |>
    filter(iter < 600) |>
    ggplot(aes(x = iter, y = sigma, color = cadena)) + 
    geom_path() + sin_leyenda + sin_lineas + 
    ggtitle(paste("Rhat: ", round((diagnosticos.rhat.short |> pull(R_hat))[2], 3), sep = "")) + 
    annotate("rect", xmin = 0, xmax = 450, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("rect", xmin = 0, xmax = 300, ymin = -Inf, ymax = Inf, alpha = .2) + 
    annotate("text", x = c(150, 375, 525),
             y = rep(5, 3), 
             label = c("burn-in", "sub 1", "sub 2"))

  g.mu / g.sigma
#+end_src
#+caption: Diágnostico de reducción de escala. Observaciones: parece estar bien. 
#+RESULTS:
[[file:../images/diagnosticos-rhat-cantantes-estacionario.jpeg]]

#+BEGIN_NOTES
Problemas con $\hat{R}$. El estimador de reducción de escala funciona bien para
monitorear estimadores y cantidades de interés basados en medias y varianzas, o
bien, cuando la distribución es simétrica y cercana a una Gaussiana. Es decir,
colas ligeras. Sin embargo, para percentiles, o distribuciones lejos del
supuesto de normalidad no es un buen indicador. Es por esto que también se
recomienda incorprorar transformaciones que nos permitan generar un buen
estimador. Puedes leer mas de esto aqui: citep:Vehtari2021a. 
#+END_NOTES

** Número efectivo de simulaciones

Queremos que los recursos que hemos asignado a generar simulaciones sean
representativos de la distribución objetivo. Si las $n$ simulaciones dentro de cada cadena en verdad son
realizaciones independientes entonces la estimación de $B$ sería un estimador insesgado 
de $\mathbb{V}(\phi)$.

#+REVEAL: split
En esta situación tendríamos $n \cdot m$ realizaciones de la distribución que
queremos simular. Sin embargo, la correlación entre las muestras hacen que $B$
sea mayor que $\mathbb{V}(\phi)$ en promedio.

#+REVEAL: split
Una manera para definir el tamaño efectivo de simulaciones es por medio del estudio
del estimador
\begin{align}
\bar{\phi}_{\cdot\cdot} \approx \mathbb{E}(\phi)\,.
\end{align}
Del cual podemos derivar que
$$\mathbb{V}(\bar{\phi}_{\cdot\cdot}) = \frac{\mathbb{V}(\phi)}{m\cdot n}\,.$$

#+REVEAL: split
El problema es que la correlación en las cadenas implica el denominador ($m\cdot n$)
realmente sea una fracción del total de muestras, digamos $\lambda$. De tal forma que 
el número efectivo de simulaciones es 
$$\mathsf{ESS} = \lambda \cdot (m \, n)\,,$$
donde
$$ \lambda = \frac{1}{\sum_{t = -\infty}^\infty \rho_t} = \frac{1}{1 + 2 \sum_{t = 1}^\infty  \rho_t}\,.$$


#+REVEAL: split
El término $\rho_t$ denota la *auto-correlación* con diferencia en $t$ unidades de tiempo.

#+REVEAL: split
#+attr_latex: :options [Autocorrelación]
#+begin_definition    
La autocovarianza y autocorrelación de una serie temporal *estacionaria* $\{Y_t : t = 0, \ldots\}$ están definidas (respectivamente) como
\begin{align}
C_\tau = \mathbb{E}[(Y_{t+\tau} - \mu ) (Y_t - \mu)], \qquad \rho_\tau = \frac{\mathbb{E}[(Y_{t+\tau} - \mu ) (Y_t - \mu)]}{\sigma^2}\,.
\end{align}
#+end_definition


#+REVEAL: split
#+attr_latex: :options [Estimador de autocorrelación]
#+begin_definition    
La función de autocorrelación se estima utilizando
\begin{align}
\hat C_\tau = \frac{1}{T- \tau} \sum_{t = 1}^{T - \tau} (Y_{t + \tau} - \hat \mu)( Y_{t} - \hat \mu), \qquad \hat \rho_\tau = \frac{\hat C_\tau}{\hat C_0}\,.
\end{align}
#+end_definition

#+REVEAL: split
#+attr_latex: :options [Variograma]
#+begin_definition
El variograma de una serie temporal *estacionaria* $\{Y_t : t = 0, \ldots\}$ está definido como
\begin{align}
V_\tau = \mathbb{E}[(Y_{t+\tau} - Y_t)^2]\,.
\end{align}

*Nota* que $V_\tau = C_0 - C_\tau$.
#+end_definition

#+REVEAL: split
Regresando a nuestro contexto... para estimar $\rho_t$ partimos de nuestro estimador $\hat{\mathbb{V}}(\phi)^+;$
y utilizamos el *variograma* $V_t$ para ~cada retraso~ $t$
$$V_t = \frac{1}{m (n - t)} \sum_{j = 1}^m \sum_{i = t + 1}^n (\phi_{i,j} - \phi_{i-t, j})^2\,.$$

#+REVEAL: split
Utilizando la igualdad $\mathbb{E}(\phi_{i} - \phi_{i-t})^2 = 2 (1 - \rho_t) \mathbb{V}(\phi)$, podemos estimar
$$\hat \rho_t = 1 - \frac{V_t}{2 \, \hat{\mathbb{V}}(\phi)^+} \, . $$

#+BEGIN_NOTES
La mayor dificultad que presenta el estimador es considerar *todos* los retrasos
posibles. Eventualmente agotaremos la longitud de las cadenas para ello. Por
otro lado, para $t$  eventualmente grande nuestros estimadores del variograma
$V_t$ serán muy ruidosos (¿por qué?). En la práctica truncamos la serie de
acuerdo a las observaciones citep:Geyer2002. La serie tiene la propiedad de que para
cada par $\rho_{2 t} + \rho_{2 t + 1} > 0$. Por lo tanto, la serie se trunca 
cuando observamos $\hat \rho_{2 t} + \hat \rho_{2 t + 1} < 0$ para dos retrasos
sucesivos.
#+END_NOTES

Si denotamos por $T$ el *tiempo de paro*, el estimador para el número efectivo de
simulaciones es
$$\widehat{\mathsf{ESS}} = \frac{m \, n}{1 + 2 \sum_{t = 1}^T \hat  \rho_t}\,.$$

#+REVEAL: split
El ~tamaño efectivo de simulaciones~ nos ayuda a monitorear lo siguiente. Si las
simulaciones fueran independientes $\textsf{ESS}$ sería el número total de
simulaciones; sin embargo, las simulaciones de MCMC suelen estar
correlacionadas, de modo que cada iteración de MCMC es menos informativa que si
fueran independientes.

#+REVEAL: split
Por ejemplo si graficaramos simulaciones independientes, esperaríamos valores de 
autocorrelación chicos:

#+HEADER: :width 900 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/autocorrelacion-indep.jpeg :exports results :results output graphics file
  library(forecast)
  ggAcf(rgamma(1000,1,1)) + sin_lineas
#+end_src

#+RESULTS:
[[file:../images/autocorrelacion-indep.jpeg]]

#+REVEAL: split
Sin embargo, los valores que simulamos tienen el siguiente perfil de
autocorrelación:

#+HEADER: :width 900 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/autocorrelacion-metropolishastings.jpeg :exports results :results output graphics file
  ggAcf(muestras.cantantes$mu) + sin_lineas +
  ggtitle("Series: mu (modelo cantantes)")
#+end_src

#+RESULTS:
[[file:../images/autocorrelacion-metropolishastings.jpeg]]


#+REVEAL: split
Podemos utilizar la función ~posterior::ess_basic~.
#+begin_src R :exports results :results org
  library(posterior)
  tibble(ESS  = round(ess_basic(muestras.cantantes$mu)),
    N.M   = nrow(muestras.cantantes),
    ratio    = ess_basic(muestras.cantantes$mu)/nrow(muestras.cantantes),
    sigma = ess_basic(muestras.cantantes$sigma)/nrow(muestras.cantantes),
    accept = mean(muestras.cantantes$V1))
#+end_src
#+caption: Fracción $\mathsf{ESS}/nm$ para la simulación de la posterior los cantantes de ópera. 
#+RESULTS:
#+begin_src org
# A tibble: 1 × 5
    ESS   N.M  ratio  sigma accept
  <dbl> <int>  <dbl>  <dbl>  <dbl>
1   146  5000 0.0292 0.0482   0.69
#+end_src


#+begin_src R :exports results :results org
  ### Actualización del muestreador  -----------------------------
  set.seed(108727)
  objetivo <- ModeloNormal$new(cantantes$estatura_cm)
  muestreo <- ModeloNormalMultivariado$new(c(0,0), 3 * diag(2))
  mcmc <- crea_metropolis_hastings(objetivo, muestreo)

  muestras.cantantes <-  mcmc(5000, c(175, 6.5)) |>
    as_tibble() |>
    mutate(mu = V2, sigma = V3, iter = 1:n())

  tibble(ESS  = round(ess_basic(muestras.cantantes$mu),0),
    N.M   = nrow(muestras.cantantes),
    ratio    = ess_basic(muestras.cantantes$mu)/nrow(muestras.cantantes),
    sigma = ess_basic(muestras.cantantes$sigma)/nrow(muestras.cantantes),
    accept = mean(muestras.cantantes$V1))
#+end_src
#+caption: Fracción $\mathsf{ESS}/nm$ para la simulación (calibrada) de la posterior los cantantes de ópera. 
#+RESULTS:
#+begin_src org
# A tibble: 1 × 5
    ESS   N.M  ratio sigma accept
  <dbl> <int>  <dbl> <dbl>  <dbl>
1   441  5000 0.0882 0.146  0.380
#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 400 :R-dev-args bg="transparent"
#+begin_src R :file images/autocorrelacion-metropolishastings-rechazo.jpeg :exports results :results output graphics file
  ggAcf(muestras.cantantes$mu) + sin_lineas +
    ggtitle("Series: mu (modelo cantantes)")
#+end_src
#+caption: Perfil de correlación para la simulación calibrada. 
#+RESULTS:
[[file:../images/autocorrelacion-metropolishastings-rechazo.jpeg]]

** Relación con error Monte Carlo

Conocer el número efectivo de simulaciones nos permite calcular el error
estándar de una aproximación Monte Carlo por medio de expresiones como
\begin{align}
\mathsf{SE}\left(\hat{\pi}^{\mathsf{MCMC}}_{\mathsf{ESS}}(f)\right) \approx \left(  \frac{\hat{\mathbb{V}}_\pi(f)}{\widehat{\mathsf{ESS}}} \right)^{\frac12}\,.
\end{align}

#+BEGIN_NOTES
citet:Geyer2002 menciona que realizar estimaciones puntuales sin medidas de
incertidumbre es el /deber/ ser de un analista/estadístico. Esto es,
independientemente de si la técnica es Bayesiana o frecuentista. El artículo de
[[citet:Flegal2008]] también discute la importancia de reportar errores estándar.
#+END_NOTES

* El estado del arte

#+REVEAL: split
El método de Metropolis-Hastings es muy flexible y existe una colección
numerable de versiones que pueden ser empleadas en contextos muy particulares.
Una buena referencia que incluye métodos de simulación por medio de cadenas de
Markov se encuentra en citep:Liu2004, donde incluso se pueden encontrar
generalizaciones con ~transiciones Markovianas asimétricas~ y extensiones a ~problemas de
dimensión variable~. El libro citep:Brooks2011 presenta el estado del arte al 2010.

#+REVEAL: split
El cómputo Bayesiano se popularizó con el muestreador de Gibbs. En particular,
el avance en teoría de grafos para representar una distribución conjunta como un
Grafo Acíclico Dirigido (DAG) que se implementó en software como ~BUGS~ o [[https://www.mrc-bsu.cam.ac.uk/software/bugs/the-bugs-project-winbugs/][WinBUGS]].
Pueden consultar el libro de citet:Kruschke2014 para su explicación.

#+REVEAL: split
La desventaja del muestreador de Gibbs es que tiende a ser muy lento en
problemas de tamaño grande. Ha habido estrategias que aceleran la simulación
aunque al ~costo de utilizar aproximaciones~. Estas estrategias han sido
materializadas en lenguajes de programación mas generales como
[[https://dotnet.github.io/infer/][Infer.NET]].

#+REVEAL: split
[[http://mcmc-jags.sourceforge.net][JAGS]] (Just Another Gibbs Sampler), es 
una generalización donde se implementan métodos MCMC para generar simulaciones
de distribuciones posteriores. Los paquetes ~rjags~ y ~R2jags~ permiten ajustar
modelos en JAGS desde ~R~ citep:Hornik2003. Es muy fácil utilizar estos
programas pues uno simplemente debe especificar las distribuciones iniciales, la
verosimilitud y los datos observados. Igual el libro de citet:Kruschke2014. 

#+REVEAL: split
Al depender de gradientes para construir propuestas para las cadenas de Markov
ha sido natural el desarrollo de herramientas de muestreo basadas en
diferenciadores automáticos. Por ejemplo, [[https://pyro.ai/][Pyro]] utiliza [[https://pytorch.org/][PyTorch]]. Tenemos también
[[https://www.tensorflow.org/probability][Tensorflow Probability]] que utiliza ~Tensorflow~. [[https://docs.pymc.io/][Pymc]] (antes Pymc3) utiliza Theano
(ahora llamado [[https://github.com/aesara-devs/aesara][Aesara]]). [[https://github.com/pyro-ppl/numpyro][NumPyro]] utiliza ~numpy~ y [[https://github.com/google/jax][JAX]] como /backend/. 

#+REVEAL: split
[[https://docs.pymc.io/][Pymc]] es un muestreador ~híbrido~ que permite utilizar Metropolis-Hastings, Gibbs
y HMC para la simulación de la posterior citep:Salvatier2016. También es
mucho más flexible y brinda muestreadores más modernos basados en particulas e
información de primer orden (gradientes).

#+REVEAL: split
Además, hay herramientas que utilizan las librerías de muestreo para análisis
específicos.  Por ejemplo, tenemos ~cmdstanarm~ ajusta *modelos de regresión*
utilizando ~Stan~ como /backend/. La herramienta de Facebook, ~Prophet~, utiliza ~Stan~
(ver [[https://statmodeling.stat.columbia.edu/2017/03/01/facebooks-prophet-uses-stan/][aqui]]) como /backend/ y se especializa en *series de tiempo*. [[https://github.com/IvanYashchuk/fenics-pymc3][fenics-pymc3]] se
especializa en soluciones de *ecuaciones diferenciales* escritas en
~FEniCS~. También tenemos [[https://github.com/hvasbath/beat][beat]] para *análisis probabilístico de terremotos* y
[[https://github.com/exoplanet-dev/exoplanet][exoplanet]] para series de tiempo en *astronomía*. Por supuesto, no podía faltar una
integración ~scikit~ que se llama [[https://www.pymc-learn.org/][Pymc-Learn]].

#+REVEAL: split
Existen otras alternativas para construir cadenas de Markov. Por ejemplo, hay
algoritmos que buscan evolucionar una colección de muestras de $\theta$ como un
enjambre que se comunican entre si para generar una caminata aleatoria en el
espacio del soporte de la distribución. Ejemplos de éstos son el ~t-walk~
citep:Christen2010 o un ensamble de cadenas linealmente relacionadas como en la
herramienta de ~emcee~  citep:Foreman-Mackey2013, 

#+REVEAL: split
Finalmente, hay muchos mas mecanismos que tienen como objetivo aproximar la
distribución posterior. En problemas donde la verosimilitud es
~computacionalmente costosa~ existen alternativas para crear aproximaciones. El
artículo citep:Garbuno-Inigo2019 provee de una alternativa utilizando una
combinación de técnicas bien establecidas (difusiones Langevin, ensamble de
partículas interactivas y filtros de Kalman).


bibliographystyle:abbrvnat
bibliography:references.bib

#+begin_src R :exports results :results org 
  sessionInfo() 
#+end_src

#+RESULTS:
#+begin_src org
R version 4.3.1 (2023-06-16)
Platform: x86_64-apple-darwin20 (64-bit)
Running under: macOS Sonoma 14.1

Matrix products: default
BLAS:   /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib 
LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

time zone: America/Mexico_City
tzcode source: internal

attached base packages:
[1] stats     graphics  grDevices datasets  utils     methods   base     

other attached packages:
 [1] posterior_1.4.1 forecast_8.21   R6_2.5.1        mvtnorm_1.2-2  
 [5] scales_1.2.1    patchwork_1.1.2 lubridate_1.9.2 forcats_1.0.0  
 [9] stringr_1.5.0   dplyr_1.1.2     purrr_1.0.1     readr_2.1.4    
[13] tidyr_1.3.0     tibble_3.2.1    ggplot2_3.4.2   tidyverse_2.0.0

loaded via a namespace (and not attached):
 [1] tensorA_0.36.2       utf8_1.2.3           generics_0.1.3      
 [4] renv_1.0.0           stringi_1.7.12       lattice_0.21-8      
 [7] hms_1.1.3            magrittr_2.0.3       grid_4.3.1          
[10] timechange_0.2.0     backports_1.4.1      nnet_7.3-19         
[13] fansi_1.0.4          abind_1.4-5          cli_3.6.1           
[16] rlang_1.1.1          crayon_1.5.2         munsell_0.5.0       
[19] withr_2.5.0          tools_4.3.1          parallel_4.3.1      
[22] tzdb_0.4.0           checkmate_2.2.0      colorspace_2.1-0    
[25] curl_5.0.1           vctrs_0.6.3          matrixStats_1.0.0   
[28] zoo_1.8-12           lifecycle_1.0.3      tseries_0.10-54     
[31] urca_1.3-3           pkgconfig_2.0.3      pillar_1.9.0        
[34] gtable_0.3.3         glue_1.6.2           quantmod_0.4.24     
[37] Rcpp_1.0.11          lmtest_0.9-40        tidyselect_1.2.0    
[40] farver_2.1.1         nlme_3.1-162         xts_0.13.1          
[43] labeling_0.4.2       timeDate_4022.108    fracdiff_1.5-2      
[46] compiler_4.3.1       quadprog_1.5-8       distributional_0.3.2
[49] TTR_0.24.3
#+end_src

