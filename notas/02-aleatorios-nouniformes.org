#+TITLE: EST-24107: Simulación
#+AUTHOR: Prof. Alfredo Garbuno Iñigo
#+EMAIL:  agarbuno@itam.mx
#+DATE: ~Números aleatorios no uniformes~
#+STARTUP: showall
:LATEX_PROPERTIES:
#+SETUPFILE: ~/.emacs.d/templates/latex/handout.org
#+EXPORT_FILE_NAME: ../docs/02-aleatorios-nouniformes.pdf
:END:
#+PROPERTY: header-args:R :session transformacion :exports both :results output org :tangle ../rscripts/02-aleatorios-nouniformes.R :mkdirp yes :dir ../ 
#+EXCLUDE_TAGS: toc

#+BEGIN_NOTES
*Profesor*: Alfredo Garbuno Iñigo | Otoño, 2023 | Números no uniformes.\\
*Objetivo*: En esta sección del curso veremos métodos para generar números aleatorios de distribuciones un poco mas generales que una distribución uniforme. En particular veremos algunos ejemplos de distribuciones continuas, discretas y mezclas. Además veremos una prueba de uniformidad adicional a la prueba Kolmogorov-Smirnov.\\
*Lectura recomendada*: Capítulo 2 de [[citep:Robert2013a]]. El capítulo 3 de citep:Ross2013 presenta los métodos para generar números aleatorios gaussianos. 
#+END_NOTES

#+begin_src R :exports none :results none
  ## Setup ---------------------------------------------------------------------
  library(tidyverse)
  library(patchwork)
  library(scales)

  ## Cambia el default del tamaño de fuente 
  theme_set(theme_linedraw(base_size = 25))

  ## Cambia el número de decimales para mostrar
  options(digits = 4)
  ## Problemas con mi consola en Emacs
  options(pillar.subtle = FALSE)
  options(rlang_backtrace_on_error = "none")
  options(crayon.enabled = FALSE)
  options(width=80)

  ## Para el tema de ggplot
  color.itam  <- c("#00362b","#004a3b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f", NA)
  sin_leyenda <- theme(legend.position = "none")
  sin_ejes <- theme(axis.ticks = element_blank(), axis.text = element_blank())
  sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
#+end_src


* Table of Contents                                                             :toc:
:PROPERTIES:
:TOC:      :include all  :ignore this :depth 3
:END:
:CONTENTS:
- [[#generación-de-variables-no-uniformes][Generación de variables no uniformes]]
  - [[#método-de-la-transformada-inversa][Método de la transformada inversa]]
  - [[#generalización][Generalización]]
- [[#distribuciones-continuas][Distribuciones continuas]]
  - [[#variables-chi2][Variables $\chi^2$]]
  - [[#variables-gamma][Variables gamma]]
  - [[#variables-beta][Variables beta]]
    - [[#código][Código:]]
    - [[#usando-distributions][Usando distributions]]
  - [[#variables-gaussianas-correlacionadas][Variables Gaussianas correlacionadas]]
- [[#distribuciones-discretas][Distribuciones discretas]]
  - [[#binomial][Binomial]]
  - [[#poisson][Poisson]]
- [[#mezclas][Mezclas]]
  - [[#binomial-negativa][Binomial negativa]]
  - [[#beta-binomial][Beta Binomial]]
  - [[#mezclas-gaussianas][Mezclas Gaussianas]]
  - [[#comentarios][Comentarios]]
- [[#prueba-chi2][Prueba $\chi^2$]]
  - [[#procedimiento-de-la-prueba-chi2][Procedimiento de la prueba $\chi^2$]]
    - [[#pregunta][Pregunta:]]
  - [[#aplicación-de-la-prueba][Aplicación de la prueba]]
  - [[#aplicación-de-pruebas][Aplicación de pruebas]]
- [[#distribución-normal][Distribución normal]]
  - [[#utilizando-el-teorema-del-límite-central][Utilizando el teorema del límite central]]
  - [[#el-método-de-box-müller][El método de Box-Müller]]
  - [[#método-de-marsaglia][Método de Marsaglia]]
- [[#conclusiones-y-comentarios][Conclusiones y comentarios]]
:END:

* Generación de variables no uniformes                           

~R~, por ejemplo, tiene distintos generadores de variables aleatorias. Un catálogo nos muestra que podemos
generar mucho mas variables que solamente una variable uniforme.

#+DOWNLOADED: screenshot @ 2022-08-15 19:03:50
#+attr_html: :width 1200 :align center
#+attr_latex: :width .95 \linewidth
#+caption: Catálogo de distribuciones en ~R~. 
[[file:images/20220815-190350_screenshot.png]]

#+REVEAL: split
En esta sección exploraremos los mecanismos tradicionales para generar números
pseudo-aleatorios de distribuciones un poco mas generales que una distribución
uniforme.

#+REVEAL: split

** Método de la transformada inversa

De su curso de cálculo de probabilidades se acordarán que si tenemos una
variable aleatoria $X$ y una función de acumulación $\mathbb{P}$. Si utilizamos $U =
\mathbb{P}(X)$, con $U$ una variable aleatoria con distribución $\mathsf{U}(0,1)$. Entonces, $X \sim \mathbb{P}$. 

#+attr_latex:
#+begin_exercise
Considera $X \sim \mathsf{Exp}(1)$, de tal forma que $\mathbb{P}(x) = 1 - e^{-x}$. Si sólo sabemos generar números uniformes, ¿cómo generarías números de $X$?   
#+end_exercise

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/exp-comparison.jpeg :exports results :results output graphics file
  g1 <- tibble(x = rexp(1000, 1)) |>
    ggplot(aes(x)) +
    geom_histogram() + sin_lineas +
    xlim(0, 8) + ggtitle("Exponencial R")

  g2 <- tibble(u = runif(1000),
               x = -log(u)) |>
    ggplot(aes(x)) +
    geom_histogram() + sin_lineas +
    xlim(0, 8) + ggtitle("Exponencial = f(Uniforme)")

  g1 + g2
#+end_src
#+CAPTION: Histogramas de simulaciones utilizando el método directo (izquierda) contra el de la transformada inversa (derecha).
#+RESULTS:
[[file:../images/exp-comparison.jpeg]]

#+begin_exercise
Considera las siguientes distribuciones:
1. Logística con $\mu, \beta$ y función de acumulación
   \begin{align}
   \mathbb{P}(x) = \frac{1}{1 + e^{-(x - \mu)/\beta}}\,.
   \end{align}
2. Cauchy con parámetros $\mu, \beta$ y función de acumulación
   \begin{align}
   \mathbb{P}(x) = \frac{1}{2} + \frac{1}{\pi} \mathsf{arctan}((x - \mu)/\beta)\,.
   \end{align}

Genera números aleatorios con valores $\mu = 1$, $\beta = 2$ y comenta el rol de
cada parámetro en cada distribución.
#+end_exercise

** Generalización

En el caso anterior asumimos que la función inversa de la función de acumulación
existe. Sin embargo, no siempre es el caso. Necesitamos definir lo siguiente.

#+attr_latex: :options [Función inversa generalizada]
#+begin_definition
Sea $X$ una variable aleatoria con función de acumulación $\mathbb{P}$. La función inversa generalizada está definida como
\begin{align}
\mathbb{P}^{-1}(u) = \inf \{ x | F(x) \geq u\}\,. 
\end{align}
#+end_definition

#+attr_latex: :options [De la Función Inversa]
#+begin_theorem
Sea $\mathbb{P}$ una función de distribución. Sea $\mathbb{P}^{-1}(\cdot)$ la
función inversa generalizada de $\mathbb{P}$ y $U \sim \mathsf{U}(0,1)$. Entonces, la variable aleatoria $X = \mathbb{P}^{-1}(U)$ tiene distribución
$\mathbb{P}$.
#+end_theorem

* Distribuciones continuas

Hay situaciones donde generar números aleatorios puede ser extremadamente
sencillo pues existe una relación entre la distribución que queremos generar y
ciertos componentes fáciles de simular.

** Variables $\chi^2$

Recordarán de su curso de cálculo de probabilidades que si $X_i \sim \mathsf{Exp}(1)$ entonces
\begin{align}
Y = 2 \sum_{j = 1}^{\nu}X_j \sim \chi_{2 \nu}^2\,, 
\end{align}
con $\nu \in \mathbb{N}$.

** Variables gamma

También recordarán que si $X_i \sim \mathsf{Exp}(1)$ entonces
\begin{align}
Y = \beta \sum_{j = 1}^{\alpha} X_j \sim \mathsf{Gamma}(\alpha, \beta)\,,
\end{align}
con $\alpha \in \mathbb{N}$.

** Variables beta

También recordarán que si $X_i \sim \mathsf{Exp}(1)$ entonces
\begin{align}
Y = \frac{\sum_{j = 1}^{\alpha} X_j}{\sum_{j = 1}^{\alpha + \beta}X_j} \sim \mathsf{Beta}(\alpha, \beta)\,,
\end{align}
con $\alpha, \beta \in \mathbb{N}$.

#+begin_exercise
Prueba las igualdades anteriores.
#+end_exercise

*** Código:

#+begin_src R :exports none :results none
  ## Ejemplo: Chi-cuadrada (6) =================================================
#+end_src

Por ejemplo, podemos utilizar el siguiente código para generar variables de una $\chi^2_6$ . 

#+begin_src R :exports both :results org
  set.seed(108)
  U <- runif(3 * 10^4)      # Genera uniformes
  U <- matrix(U, nrow = 3)  # Transforma a matriz
  X <- -log(U)              # Transforma a exponenciales
  X <- 2 * apply(X, 2, sum) # Suma los tres renglones
  summary(X) 
#+end_src

#+RESULTS:
#+begin_src org
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.151   3.505   5.420   6.064   7.926  27.904
#+end_src

#+REVEAL: split

A partir de la versión 4.1.1 ~R~ cuenta con un operador especial (~|>~) llamado ~pipe~ el
cual permite /anidar/ ciertas funciones y evitar la asignación repetitiva de
variables. 

#+begin_src R :exports both :results org
  set.seed(108)
  runif(3 * 10^4) |>        # Genera uniformes 
    matrix(nrow = 3) |>     # Transforma a matriz
    log() |>                # Calcula logaritmos
    apply(2, function(x){-2 * sum(x)} ) |> 
    summary()
#+end_src

#+RESULTS:
#+begin_src org
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.151   3.505   5.420   6.064   7.926  27.904
#+end_src

*** Usando ~distributions~

La idea de ~distributions~ es definir instancias$^\dagger$ de variables aleatorias. Esto es,
definir una variable aleatoria de una familia paramétrica utilizando un valor definido
de los parámetros que la definen. Por ejemplo, una $\mathsf{N}(\mu, \sigma) = \mathsf{N}(0, 1)$
o una $\mathsf{U}(a,b) = \mathsf{U}(0,1)$. 

#+begin_src R :exports both :results org :tangle no
  library(distributions)
  ## Definimos nuestra variable aleatoria
  x <- UniformRandomVariable$new()

  ## Muestreamos 10,000 por cada renglon.
  set.seed(108)
  x$sample(10**4, 3) |>      
    log() |>              
    apply(2, function(x){-2 * sum(x)} ) |> 
    summary()
#+end_src

#+RESULTS:
#+begin_src org
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.151   3.505   5.420   6.064   7.926  27.904
#+end_src



** Variables Gaussianas correlacionadas

Supongamos que queremos generar un par de variables $X \in \mathbb{R}^2$ de tal forma que
\begin{align}
X \sim \mathsf{N}\left( 0, \Sigma \right)\,,
\end{align}
donde $\Sigma_{ii} = 1$ para $i \in \{1,2\}$ y $\Sigma_{ij} = \rho$ con $i\neq
j$. Supongamos que sólo sabemos generar números aleatorios $\mathsf{N}(0,1)$.

#+REVEAL: split
¿Cómo generaríamos los vectores aleatorios que necesitamos?

#+REVEAL: split
¿Qué saben de propiedades matriciales de vectores aleatorios?

#+begin_src R :exports none :results none
  ## Ejemplo: Vectores Gaussianos ==============================================
#+end_src

#+REVEAL: split
#+begin_src R :exports both :results org 
  set.seed(108)
  Sigma <- diag(2); Sigma[1,2] <- .75; Sigma[2,1] <- .75;
  L <- chol(Sigma)

  Z <- rnorm(2 * 10^4)      # Generamos vectores estandar
  Z <- matrix(Z, nrow = 2)  # Reacomodamos en matriz
  X <- t(L) %*% Z           # Transformacion lineal
  cov(t(X))
#+end_src

#+RESULTS:
#+begin_src org
       [,1]   [,2]
[1,] 1.0173 0.7772
[2,] 0.7772 1.0312
#+end_src

#+BEGIN_NOTES
El operador ~%*%~ ejemplifica uno de las limitantes por diseño de ~R~. Pues no está
hecho para realizar operaciones vectoriales de manera nativa. Por ejemplo, en
~Matlab~ las operaciones son nativas y en ~python~ a través de ~numpy~ las operaciones
matriciales también (y parte de los métodos).
#+END_NOTES

* Distribuciones discretas

Ahora, veremos algunas técnicas generales para distribuciones discretas. O mejor
dicho, para generar números aleatorios con soporte en los enteros.

#+REVEAL: split
Supongamos que nuestro objetivo es poder generar de una $X\sim
\mathbb{P}_\theta$ donde $X\in \mathbb{Z}$. La estrategia es ~guardar todas las
probabilidades del soporte~. Es decir, calcular
\begin{align}
p_0 = \mathbb{P}_\theta(X \leq 0)\,, \quad p_1 = \mathbb{P}_\theta(X \leq 1)\,, \,\ldots\,,
\end{align}
generar $U \sim \mathsf{U}(0,1)$ y establecer
\begin{align}
X = k \text{ si } \, p_{k-1} < U < p_k\,.
\end{align}

** Binomial
Supongamos que nos interesa $X \sim \mathsf{Bin}(10, 0.3)$, el vector de probabilidades lo podemos calcular con la función ~pbinom(k, 10, .3)~.

#+begin_src R :exports none :results none
  ## Ejemplo: Variables binomiales =============================================
#+end_src

#+begin_src R :exports both :results org 
  k <- 1:10
  pbinom(k, 10, .3)
#+end_src

#+RESULTS:
#+begin_src org
 [1] 0.1493 0.3828 0.6496 0.8497 0.9527 0.9894 0.9984 0.9999 1.0000 1.0000
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none
  rbinomial <- function(nsamples, size, theta){
    probs <- pbinom(1:10, size, theta)
    x <- c()
    for (jj in 1:nsamples){
      u <- runif(1)
      x[jj] <- which(probs > u)[1]
    }
    return(x)
  }
#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/pseudobinomial-histogram.jpeg :exports results :results output graphics file
  set.seed(108)
  x <- rbinomial(1000, 10, .3)
  tibble(samples = x) |>
    ggplot(aes(samples)) +
    geom_histogram(aes(y = ..density..),
                   binwidth = 1,
                   color = 'white') +
    geom_line(data = tibble(x_ = 1:8, y_ = dbinom(x_, 10, .3)),
            aes(x_, y_), lwd = 1.5, lty = 2, 
            colour = "salmon") + 
    sin_lineas
#+end_src
#+CAPTION: Muestras de una binomial utilizando el método descrito anteriormente.
#+RESULTS:
[[file:../images/pseudobinomial-histogram.jpeg]]

** Poisson

Ahora supongamos que nos interesa simular de una Poisson con parámetro $\lambda = 7$.

#+REVEAL: split
¿Cuál es el soporte de una $\mathsf{Bin}(10, 0.3)$? ¿Cuál es el soporte de una $\mathsf{Poisson}(7)$?

#+REVEAL: split
#+begin_src R :exports none :results none
  ## Ejemplo: Poisson (discretas sin cota) =====================================
#+end_src
Tenemos que guardar las probabilidades
#+begin_src R :exports both :results org
  k <- 1:24
  ppois(k, 7)
#+end_src

#+RESULTS:
#+begin_src org
 [1] 0.007295 0.029636 0.081765 0.172992 0.300708 0.449711 0.598714 0.729091
 [9] 0.830496 0.901479 0.946650 0.973000 0.987189 0.994283 0.997593 0.999042
[17] 0.999638 0.999870 0.999956 0.999986 0.999995 0.999999 1.000000 1.000000
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none
  rpoisson <- function(nsamples, lambda){
    probs <- ppois(1:30, lambda)
    x <- c()
    for (jj in 1:nsamples){
      u <- runif(1)
      x[jj] <- which(probs > u)[1]
    }
    return(x)
  }
#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/pseudopoisson-histogram.jpeg :exports results :results output graphics file
  set.seed(108)
  x <- rpoisson(1000, 7)
  tibble(samples = x) |>
    ggplot(aes(samples)) +
    geom_histogram(aes(y = ..density..),
                   binwidth = 1,
                   color = 'white') +
    geom_line(data = tibble(x_ = 1:30, y_ = dpois(x_, 7)),
            aes(x_, y_), lwd = 1.5, lty = 2, 
            colour = "salmon") + 
    sin_lineas
#+end_src
#+CAPTION: Muestras de una Poisson utilizando el método descrito anteriormente.
#+RESULTS:
[[file:../images/pseudopoisson-histogram.jpeg]]

#+REVEAL: split
El problema de generar números aleatorios de la manera anterior es la necesidad de /guardar/ el vector de probabilidades. Por ejemplo, una $\mathsf{Poisson}(100)$. El intervalo $\lambda \pm 3 \sqrt{\lambda}$ es $(70, 130)$.

#+REVEAL: split
#+attr_latex: :options [Regla empírica o regla de las 3 sigmas]
#+begin_proposition
Si $X$ es una variable aleatoria con media y varianza finitas. Entonces, la probabilidad de que una realización de $X$ se encuentre a mas de 3 desviaciones estándar de la media es a lo mas $5\%$.     
#+end_proposition

* Mezclas

#+begin_src R :exports none :results none
  # Mezclas, distribuciones conjuntas y distribuciones marginales ==============
#+end_src

Otra familia de distribuciones que es muy interesante de simular son las
mezclas. Es decir, cuando podemos escribir
\begin{align}
\pi(x) = \int_\mathcal{Y} \pi(x | y) \, \pi(y) \, \text{d}y\,, \quad \text{ ó } \quad \mathbb{P}(x) = \sum_{i \in \mathcal{Y}}^{} \mathbb{P}(x | Y = i) \, \mathbb{P}(Y = i) \,,
\end{align}
siempre y cuando sea sencillo generar números aleatorios de la condicional y la marginal adicional.

#+REVEAL: split
Por ejemplo, para generar números aleatorios de una $t$ Student con $\nu$ grados
de libertad. Podemos usar la representación
\begin{align}
X | y \sim \mathsf{N}(0, \nu/y)\,, \quad Y \sim \chi^2_\nu\,.
\end{align}

** Binomial negativa

La variable aleatoria $X\sim \mathsf{BinNeg}(n, \theta)$ tiene una representación
\begin{align}
X | y \sim \mathsf{Poisson}(y)\,, \quad Y \sim \mathsf{Gamma}(n , \beta )\,,
\end{align}
donde $\beta = (1-\theta)/\theta$.

#+begin_src R :exports none :results none
  ## Ejemplo: Poisson-Gamma ====================================================
#+end_src

#+begin_src R :exports code :results none
  nsamples <- 10^4
  n <- 6; theta <- .3
  y <- rgamma(nsamples, n, rate = theta/(1-theta))
  x <- rpois(nsamples, y)
#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/rbinneg-histogram.jpeg :exports results :results output graphics file
  tibble(samples = x) |>
  ggplot(aes(x)) +
    geom_histogram(aes(y = ..density..),
                   binwidth = 1, color = "white") +
    geom_line(data = tibble(x_ = 1:60, y_ = dnbinom(x_, n, theta)),
              aes(x_, y_), lwd = 1.5, lty = 2, 
              colour = "salmon") +
    sin_lineas
#+end_src
#+CAPTION: Muestras de una binomial negativa utilizando el método de mezcla descrito anteriormente.
#+RESULTS:
[[file:../images/rbinneg-histogram.jpeg]]

#+begin_exercise
Prueba que una binomial negativa puede ser expresada como una mezcla
Poisson-Gamma.
#+end_exercise


** Beta Binomial

#+begin_src R :exports none :results none
  ## Ejemplo: Beta-Binomial ====================================================
#+end_src

Una distribución bastante conocida es la distribución Beta-Binomial la cual, como su nombre indica, está conformada por una mezcla de una $\mathsf{Bin}(n, \theta)$ y una $\mathsf{Beta}(\alpha, \beta)$. Es decir, $x \sim \mathsf{BetaBin}(n, \alpha, \beta)$ si su función de masa de probabilidad está dada por
\begin{align}
\mathbb{P}(x) = {n \choose x} \frac{B(x + \alpha, n - x + \beta)}{B(\alpha, \beta)}\,,
\end{align}
donde $B(\alpha, \beta)$ se conoce como la función Beta $B(\alpha, \beta) = (\Gamma(\alpha)\Gamma(\beta))/\Gamma(\alpha + \beta)$.

#+begin_src R :exports code :results none
  nsamples <- 10^4
  n <- 20; a <- 5; b <- 2
  theta <- rbeta(nsamples, a, b)
  x <- rbinom(nsamples, n, theta)
#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/rbetabinom-histogram.jpeg :exports results :results output graphics file
  tibble(samples = x) |>
  ggplot(aes(x)) +
    geom_histogram(aes(y = ..density..),
                   binwidth = 1, color = "white") +
    sin_lineas
#+end_src
#+CAPTION: Muestras de una beta binomial utilizando el método de mezclas descrito anteriormente.
#+RESULTS:
[[file:../images/rbetabinom-histogram.jpeg]]

** Mezclas Gaussianas

Otro modelo /famoso/ es el de una mezcla de Gaussianas. Donde tenemos $k$ posibles poblaciones cada una con proporción $\pi_k \in (0,1)$  y $\sum \pi_k = 1$. Y además, cada población tiene comportamiento distinto
\begin{align}
\pi(x | k) = \mathsf{N}(x | \mu_k, \sigma_k)\,.
\end{align}

#+REVEAL: split
#+begin_src R :exports none :results none
  ## Mezcla de Gaussianas ======================================================
#+end_src

#+begin_src R :exports code :results none
  nsamples <- 10^5
  y <- sample(1:3, size = nsamples, prob = c(.1, .7, .2), replace = TRUE)
  x <- rnorm(nsamples,
             mean = ifelse(y==1, 1, ifelse(y==2, 2, 5)),
             sd = ifelse(y==1, 0.1, ifelse(y==2, 0.5, 1)))
#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/rgaussmix-histogram.jpeg :exports results :results output graphics file
  tibble(samples = x) |>
  ggplot(aes(x)) +
    geom_histogram(aes(y = ..density..), color = "white") +
    geom_line(data = tibble(x_ = seq(0, 8, length = 500),
                            y_ = .1 * dnorm(x_, 1, 0.1) +
                                 .7 * dnorm(x_, 2, .5) +
                                 .2 * dnorm(x_, 5, 1)),
              aes(x_, y_), lwd = 1.5, lty = 2, 
              colour = "salmon") +    
    sin_lineas
#+end_src
#+CAPTION: Muestras de una mezcla Gaussiana de tres componentes utilizando el método descrito anteriormente.
#+RESULTS:
[[file:../images/rgaussmix-histogram.jpeg]]

** Comentarios

- El procedimiento para generar aleatorios de una mezcla nos permite obtener
  ~distribuciones marginales a partir de la conjunta~.
- La combinación de componentes nos permite ser selectivos en los métodos de
  generación que podamos utilizar (respetando la estructura de ~dependencia
  condicional~). 
- Por ejemplo, en el modelo
  \begin{align}
  \mathbb{P}(x) = \sum_{j \in \mathcal{Y}}^{} \mathbb{P}(X | Y = j) \mathbb{P}(Y = j)\,.
  \end{align}  


* Prueba $\chi^2$

Podemos usar otro mecanismo para probar estadísticamente si nuestros números
pseudo aleatorios siguen la distribución que deseamos.

Podemos pensar en esta alternativa como la versión ~discreta~ de la prueba ~KS~.

Lo que estamos poniendo a prueba es
\begin{align}
H_0: \mathbb{P}(x) = \mathbb{P}_0(x) \,\, \forall x\, \quad \text{ contra } \quad H_1: \mathbb{P}(x) \neq \mathbb{P}_0(x) \text{ para alguna } x\,.
\end{align}

** Procedimiento de la prueba $\chi^2$

1. Hacemos una partición del rango de la distribución supuesta en $k$
   subintervalos con límites $\{a_0, a_1, \ldots, a_k\}$, y definimos $N_j$ como
   el número de observaciones (de nuestro generador de pseudo-aleatorios) en
   cada subintervalo.

2. Calculamos la proporción esperada de observaciones en el intervalo $(a_{j-1},
   a_j]$ como
   \begin{align}
   p_j = \int_{a_{j-1}}^{a_j} \text{d} \mathbb{P}(x)\,.
   \end{align}

3. La estadística de prueba es
   \begin{align}
   \chi^2 = \sum_{j = 1}^{k} \frac{(N_j - n p_j)^2}{n p_j}\,.
   \end{align}  

#+BEGIN_NOTES
Nota que estamos comparando dos histogramas. El histograma observado que
construimos a partir de nuestros números pseudo-aleatorios contra el histograma
que esperaríamos de la distribución. ¿Puedes pensar en algún problema con esta
prueba?
#+END_NOTES

#+REVEAL: split
La visualización correspondiente sería lo siguiente. Utilizamos nuestro generador para obtener muestras. 

#+begin_src R :exports none :results none
  # Verificando distribución con Ji-cuadarada ==================================
#+end_src

#+begin_src R :exports code :results none 
  ## Esto es para poner a prueba un pseudo generador 
  rpseudo.uniform <- function(nsamples, seed = 108727){
    x0 <- seed; a <- 7**5; m <- (2**31)-1;
    x  <- x0; 
    for (jj in 2:nsamples){
      x[jj] <- (a * x[jj-1]) %% m
    }
    x/m
  }
#+end_src

#+REVEAL: split
#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/pseudo-uniform-hist.jpeg :exports results :results output graphics file
  nsamples <- 30;  nbins <- 10;
  samples <- tibble(x = rpseudo.uniform(nsamples, seed = 166136))
  samples |>
    ggplot(aes(x)) +
    geom_hline(yintercept = nsamples/nbins, color = "darkgray", lty = 2) +
    annotate("rect",
             ymin = qbinom(.95, nsamples, 1/nbins),
             ymax = qbinom(.05, nsamples, 1/nbins),
             xmin = -Inf, xmax = Inf,
             alpha = .4, fill = "gray") + 
    geom_histogram(binwidth = 1/nbins, color = "white", boundary = 1) + sin_lineas +
    coord_cartesian(xlim = c(0, 1)) + 
    ggtitle("Semilla: 166136")
#+end_src
#+CAPTION: Visualización de la realización de la prueba de $\chi^2$ descrita anteriormente.
#+RESULTS:
[[file:../images/pseudo-uniform-hist.jpeg]]


*** Pregunta:
:PROPERTIES:
:reveal_background: #00468b
:END:
¿Qué esperaríamos de nuestro estadístico $\chi^2$ si nuestro generador de pseudo-aleatorios es incorrecto?

** Aplicación de la prueba

#+begin_src R :exports none :results none
  ## Usando la noción del abogado del diablo ===================================
#+end_src

#+begin_src R :exports none :results none :tangle no
  ## Recordemos nuestro generado uniforme 
  rpseudo.uniform <- function(nsamples, seed = 108727){
    x0 <- seed; a <- 7**5; m <- (2**31)-1;
    x  <- x0; 
    for (jj in 2:nsamples){
      x[jj] <- (a * x[jj-1]) %% m
    }
    x/m
  }
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none 
  set.seed(108727)
  ## Generamos muestra de nuestro generador (congruencial lineal) 
  samples <- tibble(x = rpseudo.uniform(nsamples))

  ## Calculamos frecuencias relativas y teoricas 
  Fn <- hist(samples$x, breaks = nbins, plot = FALSE)$counts/nsamples
  F0 <- 1/nbins

  ## Calculamos nuestra referencia 
  X2.obs <- (nsamples*nbins)*sum((Fn - F0)**2)

  ## La imprimimos en pantalla 
  paste("Estadístico: ", round(X2.obs, 3))
#+end_src


#+REVEAL: split
#+begin_src R :exports code :results none
  ## Replicando experimentos =================================================== 
  experiment <- function(nsamples, nbreaks = 20){
    samples <- data.frame(x = runif(nsamples))
    Fn <- hist(samples$x, breaks = nbreaks, plot = FALSE)$counts/nsamples
    F0 <- 1/nbreaks
    X2 <- (nsamples*nbreaks)*sum((Fn - F0)**2)
    return(X2)
  }

  X2 <- c()
  for (jj in 1:5000){
    X2[jj] <- experiment(nsamples, nbins)
  }
#+end_src

#+REVEAL: split
En la [[fig-chisq-hist]] se muestra el histograma de las réplicas del estadístico
$\chi^2$ bajo el generador uniforme (lo tomamos como la distribución de la
hipótesis nula) y comparamos contra el observado (línea punteada). Adicional, se
incorpora la densidad de una $\chi^2_{k-1}$ (leáse ji-cuadrada con $k-1$ grados
de libertad) que es la distribución asintótica del estadístico.

#+HEADER: :width 900 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/chi2-histograma.jpeg :exports results :results output graphics file
  tibble(estadistica = X2) |>
    ggplot(aes(estadistica)) +
    geom_histogram(aes(y = ..density..), bins = 20, color = "white") +
    xlab(expression(chi[nbins-1]^{2})) + 
    geom_vline(xintercept = X2.obs, lty = 2, color = 'red', lwd = 1.5) +
    stat_function(fun = dchisq, args = list(df = nbins - 1), color = 'salmon', lwd = 1.5) +
    sin_lineas 
#+end_src
#+name: fig-chisq-hist
#+RESULTS:
[[file:../images/chi2-histograma.jpeg]]

#+REVEAL: split
Por lo tanto, la probabilidad de haber observador una estadístico $\chi^2$ tan extremo como el que observamos si el generador hubiera sido el que suponemos es:
#+begin_src R :exports results :results org 
  print(paste("Estadistico: ", round(X2.obs, 4), ", Probabilidad: ", mean(X2 >= X2.obs), sep =''))
#+end_src

#+RESULTS:
#+begin_src org
[1] "Estadistico: 12.6667, Probabilidad: 0.177"
#+end_src

Que podemos comparar contra el que obtenemos de una prueba "tradicional":
#+begin_src R :exports both :results org 
  counts.obs <- Fn*nsamples 
  chisq.test(counts.obs, p = rep(1, nbreaks)/nbreaks, simulate.p.value = TRUE)
#+end_src

#+RESULTS:
#+begin_src org

	Chi-squared test for given probabilities with simulated p-value (based
	on 2000 replicates)

data:  counts.obs
X-squared = 13, df = NA, p-value = 0.2
#+end_src

#+REVEAL: split
- La prueba $\chi^2$  pues usualmente no es buena cuando el número de observaciones es menor a 50.
- La prueba ~KS~ tiene mejor potencia que la prueba $\chi^2$:
  #+begin_src R :exports both :results org 
     ks.test(samples$x, "punif")
  #+end_src

  #+RESULTS:
  #+begin_src org

          Exact one-sample Kolmogorov-Smirnov test

  data:  samples$x
  D = 0.16, p-value = 0.4
  alternative hypothesis: two-sided
  #+end_src

** Aplicación de pruebas

En la práctica se utiliza una colección de pruebas pues cada una es sensible a
cierto tipo de desviaciones. La bateria de pruebas mas utilizada es la colección
de pruebas ~DieHARD~ que desarrolló [[https://en.wikipedia.org/wiki/Diehard_tests][George Marsaglia]] y que se ha ido
complementando con los años.

* Distribución normal

Hasta ahora no hemos realmente hablado de cómo generar números de una de las distribuciones mas utilizadas. La distribución normal. Si pensamos en utilizar el método de la transformada inversa tendríamos que resolver la inversa de
\begin{align}
\mathsf{Prob}\left\lbrace X \leq x \right\rbrace = \int_{- \infty}^{x} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(s - \mu)^2}{2 \sigma^2}} \text{d}s\,.
\end{align}

#+REVEAL: split
Sólo veremos ideas generales de cómo generamos números aleatorios de una distribución ~normal estándar~.

** Utilizando el teorema del límite central

Podemos utilizar una colección $U_1, \ldots, U_k \overset{\mathsf{iid}}{\sim} \mathsf{U}(a,b)$  de tal forma que tengan
\begin{align}
\mathbb{E}(U_i ) = \frac12\,, \qquad \mathbb{V}(U_i) = \frac{1}{12}\,.
\end{align}
pues por el CLT tenemos que
\begin{align}
Z = \frac{\sum_{i = 1}^{k} U_j - k/2}{\sqrt{k/12}} \overset{.}{\sim} \mathsf{N}(0,1)\,.
\end{align}

#+begin_src R :exports none :results none
  ## Numeros aleatorios normales (aprox. uniforme) =============================
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none
  nsamples <- 10^4; k <- 12
  U <- runif(k * nsamples)
  U <- matrix(U, nrow = k)
  X <- apply(U, 2, function(x){ (sum(x) - k/2)/sqrt(k/12) })
#+end_src

#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/rnormal-clt.jpeg :exports results :results output graphics file
  g1 <- tibble(samples = X) |>
    ggplot(aes(samples)) +
    geom_histogram() + sin_lineas +
    ggtitle("Utilizando uniformes")

  g2 <- tibble(samples = rnorm(nsamples)) |>
    ggplot(aes(samples)) +
    geom_histogram() + sin_lineas +
    ggtitle("Utilizando rnorm")

  g1 + g2
#+end_src

#+RESULTS:
[[file:../images/rnormal-clt.jpeg]]

#+REVEAL: split
Depende de $k$ la calidad de la aproximación. Sin embargo, siempre será un método aproximado.

** El método de Box-Müller

Este método se basa en transformación de coordenadas polares a cartesianas. Es decir,
\begin{align}
(u_1, u_2) \rightarrow (z_1, z_2)\,,
\end{align}
donde $u_i$ son uniformes independientes  y $z_i$ son normales estándar independientes.

Las coordenadas polares del vector $(z_1, z_2)$ son
\begin{align}
R ^2 = z_1^2 + z_2^2 \,, \qquad \tan \theta = \frac{z_2}{z_1}\,.
\end{align}

#+REVEAL: split
La función de densidad conjunta para $(R^2, \theta)$ es igual a
\begin{align}
\pi(r^2, \theta ) = \frac{1}{2} \frac{1}{2\pi} e^{-\frac{r^2}{2}}\,, \quad 0 < r^2 < \infty\,, \quad 0 < \theta < 2\pi\,.
\end{align}

#+REVEAL: split
Podemos notar que tenemos
\begin{align}
\pi(r^2, \theta) = \pi(r^2) \cdot \pi(\theta)\,.
\end{align}
Es decir, son ~independientes~.

#+REVEAL: split
Asi que tenemos que
\begin{gather}
r^2 \sim \mathsf{Exp}(2)\,,\\
\theta \sim \mathsf{U}(0, 2\pi)\,.
\end{gather} 

#+REVEAL: split
Lo cual podemos finalmente escribir como
\begin{align}
z_1 = \sqrt{-2\log u_1} \cos (2\pi u_2)\,,\\
z_2 = \sqrt{-2\log u_1} \sin (2\pi u_2)\,.\\
\end{align}
#+REVEAL: split
El problema de esto es que necesitamos calcular funciones complejas ($\sin(\cdot), \cos(\cdot), \sqrt{\cdot}$). 

#+begin_src R :exports none :results none
  ## Numeros normales (metodo polar)
#+end_src

#+REVEAL: split
#+begin_src R :exports code :results none 
  rnormal.bm <- function(n){
    r <- sqrt(-2 * log(runif(n)))
    theta <- runif(n, 0, 2 * pi)
    z <- matrix(0, nrow = 2, ncol = n)
    z[1,] <- r * cos(2 * pi * theta)
    z[2,] <- r * sin(2 * pi * theta)
    return(z)
  }
#+end_src

#+HEADER: :width 1200 :height 500 :R-dev-args bg="transparent"
#+begin_src R :file images/rnormal-bm.jpeg :exports results :results output graphics file
  set.seed(108)
  z <- rnormal.bm(nsamples)
  g.joint <- tibble(z1 = z[1,], z2 = z[2,]) |>
    ggplot(aes(z1, z2)) +
    geom_point() + ylab(expression(z[2])) +
    xlab(expression(z[1])) +
    sin_lineas
  g.x <- tibble(z1 = z[1,], z2 = z[2,]) |>
    ggplot(aes(z1)) +
    geom_histogram() + xlab(expression(z[1])) + 
    sin_lineas
  g.y <- tibble(z1 = z[1,], z2 = z[2,]) |>
    ggplot(aes(z2)) +
    geom_histogram() + xlab(expression(z[2])) + 
    sin_lineas

  (g.x / g.y | g.joint)

#+end_src

#+RESULTS:
[[file:../images/rnormal-bm.jpeg]]


** Método de Marsaglia

Usaremos el método de Box-Müller como punto de partida (coordenadas polares y
cartesianas). Lo que nos interesa es poder generar números aleatorios dentro del
círculo. Asi que generamos
\begin{align}
V_1 = 2 U_1 - 1\,, \quad V_2 = 2 U_2 - 2\,, 
\end{align}
a partir de $U_i \sim \mathsf{U}(0,1)$. 

#+REVEAL: split
Si generamos un punto ~dentro~ del círculo unitario nos lo quedamos como una
simulación válida. Si no, entonces repetimos la generación de los número
aleatorios uniformes.

#+REVEAL: split
Si el punto se encuentra dentro del círculo entonces extraemos sus coordenadas por medio de
\begin{gather}
S = V_1^2 + V_2^2\,,\\
z_1 = \sqrt{\frac{-2\log S}{S}} V_1\,, \qquad z_2 = \sqrt{\frac{-2\log S}{S}} V_2\,.
\end{gather}

#+REVEAL: split
El método anterior tiene probabilidad $\pi/4$ de generar un punto dentro del
círculo. Esto es, para generar 2 números gaussianos independientes usaremos en
promedio $4/\pi\approx 1.273$ iteraciones de lanzar dardos. Esto es, generar en
promedio $2.546$ números aleatorios y un cálculo de logaritmos, una raíz
cuadrada, una división y 4.546 multiplicaciones.

* Conclusiones y comentarios

- El método de la transformada inversa es útil. Sin embargo, no siempre se puede
  calcular de forma cerrada (distribución normal).
- El modelo de mezclas es muy común en aplicaciones financieras, estudios de
  mercado y problemas de clasificación (segmentación).


#+begin_src R :exports both :results org 
  sessionInfo()
#+end_src

#+RESULTS:
#+begin_src org
R version 4.3.1 (2023-06-16)
Platform: x86_64-apple-darwin20 (64-bit)
Running under: macOS Ventura 13.4.1

Matrix products: default
BLAS:   /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib 
LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

time zone: America/Mexico_City
tzcode source: internal

attached base packages:
[1] stats     graphics  grDevices datasets  utils     methods   base     

other attached packages:
 [1] scales_1.2.1    patchwork_1.1.2 lubridate_1.9.2 forcats_1.0.0  
 [5] stringr_1.5.0   dplyr_1.1.2     purrr_1.0.1     readr_2.1.4    
 [9] tidyr_1.3.0     tibble_3.2.1    ggplot2_3.4.2   tidyverse_2.0.0

loaded via a namespace (and not attached):
 [1] crayon_1.5.2     vctrs_0.6.3      cli_3.6.1        rlang_1.1.1     
 [5] stringi_1.7.12   renv_1.0.0       generics_0.1.3   labeling_0.4.2  
 [9] glue_1.6.2       colorspace_2.1-0 hms_1.1.3        fansi_1.0.4     
[13] grid_4.3.1       munsell_0.5.0    tzdb_0.4.0       lifecycle_1.0.3 
[17] compiler_4.3.1   timechange_0.2.0 pkgconfig_2.0.3  farver_2.1.1    
[21] R6_2.5.1         tidyselect_1.2.0 utf8_1.2.3       pillar_1.9.0    
[25] magrittr_2.0.3   tools_4.3.1      withr_2.5.0      gtable_0.3.3
#+end_src



bibliographystyle:abbrvnat
bibliography:references.bib


 
